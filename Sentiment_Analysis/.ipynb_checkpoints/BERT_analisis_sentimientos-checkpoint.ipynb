{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vd82CFhYntkf"
   },
   "source": [
    "# Análisis de sentimientos con BERT\n",
    "In these example we gonna use BERT model for make a binary classification, about how possitive or negative are some comments about some films. This example can be extrapolated to any case of binary classification where the input of the problem is based on natural text, where the context it's really important.\n",
    "\n",
    "We have extracted BERT model from Hugging Face page, in this big repository exists a lot of models based on text processing.\n",
    "\n",
    "[Hugging Face](https://huggingface.co/)\n",
    "\n",
    "![BERT análisis sentimientos](https://drive.google.com/uc?export=view&id=1UwciEQKNZ4SoXn_c0l31hsyZ-8jLdtVf)\n",
    "\n",
    "In this example we'll modelate a neural network one level upper, this NN recibe bert output (embeeds-> 768 dimensions) as input and generate 2 Dimension output, one per each class that we want to predict.\n",
    "\n",
    "## Installing some librarys\n",
    "First of all we should install transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10257,
     "status": "ok",
     "timestamp": 1595182614474,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "_vbubgPZnoBz",
    "outputId": "24d53394-46d5-47c6-c186-a49b15795092"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Short description for all packages that will be used in this project:\n",
    "\n",
    "- BertModel: pretrained BertModel.\n",
    "- BertTokenizer: used for transform text to Tokens that will be the input of BertModel.\n",
    "- AdamW: optimizer function for gradient descent.\n",
    "- get_linear_shedule_with_warmup: \n",
    "- torch: to realize all comput operations.\n",
    "- train_test_split: split dataset between train and test.\n",
    "- wrap: for visualize text on screen in a prety format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6222,
     "status": "ok",
     "timestamp": 1595182710408,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "Mc8cWZgOo-fw"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable definitions\n",
    "Define some parameters:\n",
    "- RANDOM_SEED: define random every time that we reexecute program will use same seed and then same results.\n",
    "- MAX_LEN: max text size processing.\n",
    "- BATCH_SIZE: size of part of input data that we will compute on the same time.\n",
    "- DATASET_PATH: dataset, this is own by each model proposal (in this case coments with labels that refers if is good or not)\n",
    "- NCLASSES: number of classes that own model will have.\n",
    "\n",
    "Select cuda device if it's avaliable, it's necessary for train if don't want to expend too much time.\n",
    "As in this example you can use googlecolab for use free GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1865,
     "status": "ok",
     "timestamp": 1595182972536,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "2YBx1vdgTGV4",
    "outputId": "8a24bee9-15e7-4873-9603-5d1240773241"
   },
   "outputs": [],
   "source": [
    "# Initialitzation\n",
    "RANDOM_SEED = 42\n",
    "MAX_LEN = 200\n",
    "BATCH_SIZE = 16\n",
    "DATASET_PATH = '/content/drive/My Drive/BERT/IMDB_Dataset.csv'\n",
    "NCLASSES = 2\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "For more commodites we will load data from drive and read it with pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34394,
     "status": "ok",
     "timestamp": 1595183077512,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "NPax5SRMUHbL",
    "outputId": "9324dbba-73b4-4ea5-8764-69285dae1069"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "df = df[0:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reducce training time, we only gonna use  10000 rows if you don't care about computing time you can remove this last part of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1595183136771,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "SypONkqHUZGA",
    "outputId": "82e3d414-4943-48e2-824b-3b877f9253e1"
   },
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(\"\\n\".join(wrap(df['review'][200])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need convert target column to binary value (0 or 1), cause algorithm needs these type of target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1492,
     "status": "ok",
     "timestamp": 1595183306570,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "UMCr2ZBvUvoX",
    "outputId": "f56691d3-28c7-49b6-9922-4bd800dc41dd"
   },
   "outputs": [],
   "source": [
    "df['label'] = (df['sentiment']=='positive').astype(int)\n",
    "df.drop('sentiment', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data\n",
    "The most important part of this example is to prepare data in a format that bert can interpretate.\n",
    "First of all we should understand what tokenization task is it. This tokenizer will convert sentences to numbers that bert will interpretate and convert to embeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "e3e30e0984d84fe293381c20dea0bb8a",
      "8aa9e224697d4d208ca495383d100ea9",
      "1c330ca1667943879b63b3f121145e0d",
      "875d8eaae29f41d0863919104b22abeb",
      "e0deb314b25e4fba83dcab638a85aad1",
      "fa3811c703cd4a0da0e6c7dfedf570dc",
      "aebf484cd1d84d958fb53e5be66b7b45",
      "fabe7b500ea74f0abf58b9801e851c44"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3909,
     "status": "ok",
     "timestamp": 1595183560075,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "yEsKKezJVZEW",
    "outputId": "0a69600c-5735-49da-a3b3-84119760b0d1"
   },
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To throw some light to tokenization, let's see one simple example about how tokenizer converts one input sentence to vector tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1414,
     "status": "ok",
     "timestamp": 1595183680949,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "LkNsk0GGWWW6",
    "outputId": "44a85379-9272-48e8-f17b-3698ff1df01a"
   },
   "outputs": [],
   "source": [
    "# Tokenization example\n",
    "sample_txt = 'I really loved that movie!'\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print('Frase: ', sample_txt)\n",
    "print('Tokens: ', tokens)\n",
    "print('Tokens numéricos: ', token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT model needs one special format data to works fine. To optimize computation time, BERT works with data vector of same lenght it's important to take care about this, cause we will add some padding tokens to maintain the same lenght. To exclude this parts of analysis we will use attention_mask to tell BERT model will know if this part is relevant or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1019,
     "status": "ok",
     "timestamp": 1595183976659,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "XDqgDsUqW0fO"
   },
   "outputs": [],
   "source": [
    "# Codification to BERT example\n",
    "encoding = tokenizer.encode_plus(\n",
    "    sample_txt,\n",
    "    max_length = 10,\n",
    "    truncation = True,\n",
    "    add_special_tokens = True,\n",
    "    return_token_type_ids = False,\n",
    "    pad_to_max_length = True,\n",
    "    return_attention_mask = True,\n",
    "    return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 972,
     "status": "ok",
     "timestamp": 1595183984043,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "QfE50cS2X8uu",
    "outputId": "e72a5df7-3fbb-4c3b-af92-c7ed91ea25e5"
   },
   "outputs": [],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing encoding example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1642,
     "status": "ok",
     "timestamp": 1595184058163,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "er5DlrQ9X-mG",
    "outputId": "8d2f1141-71e6-4812-dbff-5853e47b9560"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]))\n",
    "print(encoding['input_ids'][0])\n",
    "print(encoding['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT model needs structured data that derivate from torch.Dataset where we only need to implement 3 functions:\n",
    "- __init__: initialitzation dataset\n",
    "- __len__: return len of dataset\n",
    "- __getitem__: return data that will enter to BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1328,
     "status": "ok",
     "timestamp": 1595186242198,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "pqv-DZtzYQhm"
   },
   "outputs": [],
   "source": [
    "# DATASET creation\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "\n",
    "    def __init__(self,reviews,labels,tokenizer,max_len):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        label = self.labels[item]\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            review,\n",
    "            max_length = self.max_len,\n",
    "            truncation = True,\n",
    "            add_special_tokens = True,\n",
    "            return_token_type_ids = False,\n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt'\n",
    "            )\n",
    "\n",
    "\n",
    "        return {\n",
    "              'review': review,\n",
    "              'input_ids': encoding['input_ids'].flatten(),\n",
    "              'attention_mask': encoding['attention_mask'].flatten(),\n",
    "              'label': torch.tensor(label, dtype=torch.long)\n",
    "          } \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have clear code we define function that will create dataset from  DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1540,
     "status": "ok",
     "timestamp": 1595186245811,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "NaYe0FsoaNAv"
   },
   "outputs": [],
   "source": [
    "# Data loader:\n",
    "\n",
    "def data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = IMDBDataset(\n",
    "      reviews = df.review.to_numpy(),\n",
    "      labels = df.label.to_numpy(),\n",
    "      tokenizer = tokenizer,\n",
    "      max_len = MAX_LEN\n",
    "  )\n",
    "\n",
    "      return DataLoader(dataset, batch_size = BATCH_SIZE, num_workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all machine learning algorithms it's important to split dataset on train and test. Train dataset it's used for train model and test dataset will be the data that we will use for define how good it's our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1348,
     "status": "ok",
     "timestamp": 1595186248969,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "m9rNI4Tsa5MZ"
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size = 0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "train_data_loader = data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "It's easy to create one new model based on BERT, we conli should define our base model (BERT), and added layers. In this case we add dropout layer that will make that our model avoid overfits by hidding some neural perceptrons on each time that we load data.\n",
    "Create neural network that will be the power of our model, that will recibe BERT output as input and will have 2 neurons as output, one per each class to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 795,
     "status": "ok",
     "timestamp": 1595186916420,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "VOnca-sVbhPX"
   },
   "outputs": [],
   "source": [
    "class BERTSentimentClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(BERTSentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "    _, cls_output = self.bert(\n",
    "        input_ids = input_ids,\n",
    "        attention_mask = attention_mask\n",
    "    )\n",
    "        drop_output = self.drop(cls_output)\n",
    "        output = self.linear(drop_output)\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model and send it to device.\n",
    "\n",
    "Printing model you can see that BERT model implements a lot of layers and at the end were our two added layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4986,
     "status": "ok",
     "timestamp": 1595186924700,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "CanuNncKdXaF"
   },
   "outputs": [],
   "source": [
    "model = BERTSentimentClassifier(NCLASSES)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6-VlpvidgLp"
   },
   "source": [
    "Let's define some interesting parameters that we will use to train our model:\n",
    "- EPOCHS: number of iterations that our model will train with whole training dataset.\n",
    "- Optimizer: optimizer function.\n",
    "- total_steps: total number of examples to process.\n",
    "- scheduler: this will change our lerning rate of our optimizer to improve our error, and have more robust model.\n",
    "- loss_fn: function that we want to optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1427,
     "status": "ok",
     "timestamp": 1595186928731,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "Mansb6pfdn_H"
   },
   "outputs": [],
   "source": [
    "# ENTRENAMIENTO\n",
    "EPOCHS = 5\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train fucntion and validation function basically the same only with two diferences, when we train we should put our model to train mode, wich let us make some weight rectification, mean while when we evaluate our model run as eval mode, wich freezze our weights.\n",
    "Last  5 th rows of trrain_epoch's function are dedicated to upgrade this weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1902,
     "status": "ok",
     "timestamp": 1595187521031,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "ihoNOG0Zeuhz"
   },
   "outputs": [],
   "source": [
    "# TRAINING EPOCH\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)\n",
    "\n",
    "# EVALUATING MODEL\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each epoch train model and validate it, printing results on screen. If you want to let our alogtrithm training a lot of time and only store best of them, you should store model at the end of each epoh where the results are better, in this case lower Loss and high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1725227,
     "status": "ok",
     "timestamp": 1595189287764,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "y0aJRM-ZiXtp",
    "outputId": "4a352fee-504b-4575-8171-97fa62fc4669"
   },
   "outputs": [],
   "source": [
    "# TRAINING!!!\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch {} de {}'.format(epoch+1, EPOCHS))\n",
    "    print('------------------')\n",
    "    train_acc, train_loss = train_epoch(\n",
    "      model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train)\n",
    "    )\n",
    "    test_acc, test_loss = eval_model(\n",
    "      model, test_data_loader, loss_fn, device, len(df_test)\n",
    "    )\n",
    "    print('Training: Loss: {}, accuracy: {}'.format(train_loss, train_acc))\n",
    "    print('Validation: Loss: {}, accuracy: {}'.format(test_loss, test_acc))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This funtion recibe as input one review and predict if it's possitive or negative sentiment as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1878,
     "status": "ok",
     "timestamp": 1595189814769,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "QTG7YKnVjDA-"
   },
   "outputs": [],
   "source": [
    "def classifySentiment(review_text):\n",
    "    encoding_review = tokenizer.encode_plus(\n",
    "      review_text,\n",
    "      max_length = MAX_LEN,\n",
    "      truncation = True,\n",
    "      add_special_tokens = True,\n",
    "      return_token_type_ids = False,\n",
    "      pad_to_max_length = True,\n",
    "      return_attention_mask = True,\n",
    "      return_tensors = 'pt'\n",
    "      )\n",
    "\n",
    "    input_ids = encoding_review['input_ids'].to(device)\n",
    "    attention_mask = encoding_review['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, prediction = torch.max(output, dim=1)\n",
    "    print(\"\\n\".join(wrap(review_text)))\n",
    "    if prediction:\n",
    "    print('Predicted sentiment: 5')\n",
    "    else:\n",
    "    print('Predicted sentiment: 1')\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1698,
     "status": "ok",
     "timestamp": 1595190071968,
     "user": {
      "displayName": "Miguel Sotaquirá",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giuk6OGsE84qo69ZTgJj4B328D6GfU4etVY0q9AUg=s64",
      "userId": "08038952820764930757"
     },
     "user_tz": 300
    },
    "id": "oe0im1_alf4K",
    "outputId": "53a1e148-5007-4de5-ddcc-b5ae837710c2"
   },
   "outputs": [],
   "source": [
    "review_text = \"Avengers: Infinity War at least had the good taste to abstain from Jeremy Renner. No such luck in Endgame.\"\n",
    "\n",
    "classifySentiment(review_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOfmuLJQuUxzQsFZjQXH/bz",
   "collapsed_sections": [],
   "name": "BERT_analisis_sentimientos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1c330ca1667943879b63b3f121145e0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa3811c703cd4a0da0e6c7dfedf570dc",
      "max": 213450,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e0deb314b25e4fba83dcab638a85aad1",
      "value": 213450
     }
    },
    "875d8eaae29f41d0863919104b22abeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fabe7b500ea74f0abf58b9801e851c44",
      "placeholder": "​",
      "style": "IPY_MODEL_aebf484cd1d84d958fb53e5be66b7b45",
      "value": " 213k/213k [00:00&lt;00:00, 240kB/s]"
     }
    },
    "8aa9e224697d4d208ca495383d100ea9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aebf484cd1d84d958fb53e5be66b7b45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0deb314b25e4fba83dcab638a85aad1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e3e30e0984d84fe293381c20dea0bb8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1c330ca1667943879b63b3f121145e0d",
       "IPY_MODEL_875d8eaae29f41d0863919104b22abeb"
      ],
      "layout": "IPY_MODEL_8aa9e224697d4d208ca495383d100ea9"
     }
    },
    "fa3811c703cd4a0da0e6c7dfedf570dc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fabe7b500ea74f0abf58b9801e851c44": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
