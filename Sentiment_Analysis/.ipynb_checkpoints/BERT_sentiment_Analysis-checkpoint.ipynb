{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"BERT_analisis_sentimientos.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e7cf1da8da214ea488dd9aca5c1f8e17":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6af83df99b244f358fa0ebca293df2d1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_853fa5889c0f42fc9a03cc84a1c5911b","IPY_MODEL_f520136c2c574b1183ba2082f61cb965"]}},"6af83df99b244f358fa0ebca293df2d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"853fa5889c0f42fc9a03cc84a1c5911b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f2953231bc444bc1bae95916fccd9e91","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2b4ca677bbcd499998e02c7ee8b62dac"}},"f520136c2c574b1183ba2082f61cb965":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b0c60aed75f24a1d8a16e356e1e241d1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 213k/213k [00:02&lt;00:00, 88.6kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b80a59f761614af2a77d048567383c2b"}},"f2953231bc444bc1bae95916fccd9e91":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2b4ca677bbcd499998e02c7ee8b62dac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b0c60aed75f24a1d8a16e356e1e241d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b80a59f761614af2a77d048567383c2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b55dd42fb5b445e5a15daebdba9ec101":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a75d59d1d03e42228360367ba6baed47","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5a7882c95e0e4bcbbfd6c8d6d6d6cb91","IPY_MODEL_f43f9eb0e89843eeb3dff7edc77469a3"]}},"a75d59d1d03e42228360367ba6baed47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5a7882c95e0e4bcbbfd6c8d6d6d6cb91":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1be30f5922b84a399fd35acf65b48a4b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":29,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":29,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eaa9a4c76a804f80b0f44ca7aa3682b9"}},"f43f9eb0e89843eeb3dff7edc77469a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_81fb8048f226497a9f1db478d48cb9b3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29.0/29.0 [00:01&lt;00:00, 28.3B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6b47a52d556647ff9872fd93f0681e85"}},"1be30f5922b84a399fd35acf65b48a4b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"eaa9a4c76a804f80b0f44ca7aa3682b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"81fb8048f226497a9f1db478d48cb9b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6b47a52d556647ff9872fd93f0681e85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"76c0142ee1ac4227a994d920cc6361d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fe394895e0d74bcf94a9d5a0d107dc69","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fe2fd54fd1754c58bd5a5111f16a16b2","IPY_MODEL_912b2eb24f51478cb70524ebd7f5191c"]}},"fe394895e0d74bcf94a9d5a0d107dc69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fe2fd54fd1754c58bd5a5111f16a16b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_25a6a3ba9ea84924960b04a0085c8757","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":435797,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435797,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3201fee91c3243d8ae61020f72c03553"}},"912b2eb24f51478cb70524ebd7f5191c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a1906d7dd2124a06b4a29741cab69fa3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 436k/436k [00:00&lt;00:00, 971kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_59c6e4ae8cf54ba083c835e03b317f67"}},"25a6a3ba9ea84924960b04a0085c8757":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3201fee91c3243d8ae61020f72c03553":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1906d7dd2124a06b4a29741cab69fa3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"59c6e4ae8cf54ba083c835e03b317f67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"93945a4151bd4f75970f68c677e47804":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7311ea8dd5624cd595cec1d9092f1fd1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8b449fc63bec4e85ae9956d27f50b48f","IPY_MODEL_58101c72078e4debbeec620b673a5c79"]}},"7311ea8dd5624cd595cec1d9092f1fd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8b449fc63bec4e85ae9956d27f50b48f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cf9cb125ebec4d84a9f1b92369e228ea","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7047b54ed3eb44f9b254b711f0d6a9c7"}},"58101c72078e4debbeec620b673a5c79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b2674396e7924eb78cc76151f991926f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:00&lt;00:00, 610B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_21c60397c2dd4f0f9238028210843bec"}},"cf9cb125ebec4d84a9f1b92369e228ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7047b54ed3eb44f9b254b711f0d6a9c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b2674396e7924eb78cc76151f991926f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"21c60397c2dd4f0f9238028210843bec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"49a1682522b54d75b81439f909b61141":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cee3a917e8054873b63db48de4ed1228","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e0743d771ab146ae92b0b48702ced581","IPY_MODEL_56ae9f475c634386804597808a8a3574"]}},"cee3a917e8054873b63db48de4ed1228":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0743d771ab146ae92b0b48702ced581":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b27de67ee9d44fdd814b502fd65cab0b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":435779157,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435779157,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_07160fcf72574e358f724b70e207ea17"}},"56ae9f475c634386804597808a8a3574":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8231499765a84865b4203270ec42030f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 436M/436M [00:12&lt;00:00, 34.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b1501470638946098ead819f82f422e4"}},"b27de67ee9d44fdd814b502fd65cab0b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"07160fcf72574e358f724b70e207ea17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8231499765a84865b4203270ec42030f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b1501470638946098ead819f82f422e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"vd82CFhYntkf"},"source":["# Análisis de sentimientos con BERT\n","In these example we gonna use BERT model for make a binary classification, about how possitive or negative are some comments about some films. This example can be extrapolated to any case of binary classification where the input of the problem is based on natural text, where the context it's really important.\n","\n","We have extracted BERT model from Hugging Face page, in this big repository exists a lot of models based on text processing.\n","\n","[Hugging Face](https://huggingface.co/)\n","\n","![BERT análisis sentimientos](https://drive.google.com/uc?export=view&id=1UwciEQKNZ4SoXn_c0l31hsyZ-8jLdtVf)\n","\n","In this example we'll modelate a neural network one level upper, this NN recibe bert output (embeeds-> 768 dimensions) as input and generate 2 Dimension output, one per each class that we want to predict.\n","\n","## Installing some librarys\n","First of all we should install transformers library."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_vbubgPZnoBz","executionInfo":{"status":"ok","timestamp":1618737348684,"user_tz":-120,"elapsed":9542,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}},"outputId":"dd34662d-15c4-46c1-f02d-ed6a02455206"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 10.7MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n","\u001b[K     |████████████████████████████████| 870kB 36.9MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 37.1MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=87fb460ab54d0393572adecb1891dafec10e1fecd08c7b628d492401b9f7dfa8\n","  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rr1s3t6LEuLw"},"source":["## Imports\n","Short description for all packages that will be used in this project:\n","\n","- BertModel: pretrained BertModel.\n","- BertTokenizer: used for transform text to Tokens that will be the input of BertModel.\n","- AdamW: optimizer function for gradient descent.\n","- get_linear_shedule_with_warmup: \n","- torch: to realize all comput operations.\n","- train_test_split: split dataset between train and test.\n","- wrap: for visualize text on screen in a prety format."]},{"cell_type":"code","metadata":{"id":"Mc8cWZgOo-fw","executionInfo":{"status":"ok","timestamp":1618737354594,"user_tz":-120,"elapsed":11704,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}}},"source":["from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","import torch\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from textwrap import wrap"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IeKNQB2lEuLy"},"source":["## Variable definitions\n","Define some parameters:\n","- RANDOM_SEED: define random every time that we reexecute program will use same seed and then same results.\n","- MAX_LEN: max text size processing.\n","- BATCH_SIZE: size of part of input data that we will compute on the same time.\n","- DATASET_PATH: dataset, this is own by each model proposal (in this case coments with labels that refers if is good or not)\n","- NCLASSES: number of classes that own model will have.\n","\n","Select cuda device if it's avaliable, it's necessary for train if don't want to expend too much time.\n","As in this example you can use googlecolab for use free GPU."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2YBx1vdgTGV4","executionInfo":{"status":"ok","timestamp":1618737354595,"user_tz":-120,"elapsed":8622,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}},"outputId":"d879e8dc-aa73-48cc-a248-5cb0463ed8d7"},"source":["# Initialitzation\n","RANDOM_SEED = 42\n","MAX_LEN = 200\n","BATCH_SIZE = 16\n","DATASET_PATH = '/content/drive/My Drive/BERT/IMDB_Dataset.csv'\n","NCLASSES = 2\n","\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lDGJyBiaEuL0"},"source":["## Load data\n","For more commodites we will load data from drive and read it with pandas library"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NPax5SRMUHbL","executionInfo":{"status":"ok","timestamp":1618737380072,"user_tz":-120,"elapsed":31696,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}},"outputId":"c3888a7d-865b-4e95-81e7-751f6e0864c8"},"source":["# Load dataset\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","df = pd.read_csv(DATASET_PATH)\n","df = df[0:10000]"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6Tlltv9pEuL1"},"source":["To reducce training time, we only gonna use  10000 rows if you don't care about computing time you can remove this last part of code."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SypONkqHUZGA","executionInfo":{"status":"ok","timestamp":1618737380073,"user_tz":-120,"elapsed":1399,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}},"outputId":"5d738935-3fd5-46a4-8cbf-c49d4133f980"},"source":["print(df.head())\n","print(df.shape)\n","print(\"\\n\".join(wrap(df['review'][200])))\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n","(10000, 2)\n","Interesting and short television movie describes some of the\n","machinations surrounding Jay Leno's replacing Carson as host of the\n","Tonight Show. Film is currently very topical given the public drama\n","surrounding Conan O'Brien and Jay Leno.<br /><br />The film does a\n","good job of sparking viewers' interest in the events and showing some\n","of the concerns of the stakeholders, particularly of the NBC\n","executives. The portrayal of Ovitz was particularly compelling and\n","interesting, I thought.<br /><br />Still, many of the characters were\n","only very briefly limned or touched upon, and some of the acting\n","seemed perfunctory. Nevertheless, an interesting story.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a535Nr5yEuL2"},"source":["We need convert target column to binary value (0 or 1), cause algorithm needs these type of target column."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"UMCr2ZBvUvoX","executionInfo":{"status":"ok","timestamp":1618737381542,"user_tz":-120,"elapsed":984,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}},"outputId":"53f5d63d-cf6a-4eb1-f8fe-7d170d0e5154"},"source":["df['label'] = (df['sentiment']=='positive').astype(int)\n","df.drop('sentiment', axis=1, inplace=True)\n","df.head()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review  label\n","0  One of the other reviewers has mentioned that ...      1\n","1  A wonderful little production. <br /><br />The...      1\n","2  I thought this was a wonderful way to spend ti...      1\n","3  Basically there's a family where a little boy ...      0\n","4  Petter Mattei's \"Love in the Time of Money\" is...      1"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"Xmr1lRykEuL3"},"source":["## Preparing data\n","The most important part of this example is to prepare data in a format that bert can interpretate.\n","First of all we should understand what tokenization task is it. This tokenizer will convert sentences to numbers that bert will interpretate and convert to embeeds."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":164,"referenced_widgets":["e7cf1da8da214ea488dd9aca5c1f8e17","6af83df99b244f358fa0ebca293df2d1","853fa5889c0f42fc9a03cc84a1c5911b","f520136c2c574b1183ba2082f61cb965","f2953231bc444bc1bae95916fccd9e91","2b4ca677bbcd499998e02c7ee8b62dac","b0c60aed75f24a1d8a16e356e1e241d1","b80a59f761614af2a77d048567383c2b","b55dd42fb5b445e5a15daebdba9ec101","a75d59d1d03e42228360367ba6baed47","5a7882c95e0e4bcbbfd6c8d6d6d6cb91","f43f9eb0e89843eeb3dff7edc77469a3","1be30f5922b84a399fd35acf65b48a4b","eaa9a4c76a804f80b0f44ca7aa3682b9","81fb8048f226497a9f1db478d48cb9b3","6b47a52d556647ff9872fd93f0681e85","76c0142ee1ac4227a994d920cc6361d8","fe394895e0d74bcf94a9d5a0d107dc69","fe2fd54fd1754c58bd5a5111f16a16b2","912b2eb24f51478cb70524ebd7f5191c","25a6a3ba9ea84924960b04a0085c8757","3201fee91c3243d8ae61020f72c03553","a1906d7dd2124a06b4a29741cab69fa3","59c6e4ae8cf54ba083c835e03b317f67"]},"id":"yEsKKezJVZEW","executionInfo":{"status":"ok","timestamp":1618737386467,"user_tz":-120,"elapsed":3802,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}},"outputId":"a59bcd60-a254-4644-e872-25405e8e2efc"},"source":["# TOKENIZATION\n","PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e7cf1da8da214ea488dd9aca5c1f8e17","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b55dd42fb5b445e5a15daebdba9ec101","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76c0142ee1ac4227a994d920cc6361d8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uLsislRREuL3"},"source":["To throw some light to tokenization, let's see one simple example about how tokenizer converts one input sentence to vector tokens."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LkNsk0GGWWW6","executionInfo":{"status":"ok","timestamp":1618737386954,"user_tz":-120,"elapsed":2534,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}},"outputId":"c0e348b3-e747-4ccc-fffe-0fa1cb71460e"},"source":["# Tokenization example\n","sample_txt = 'I really loved that movie!'\n","tokens = tokenizer.tokenize(sample_txt)\n","token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","print('Frase: ', sample_txt)\n","print('Tokens: ', tokens)\n","print('Tokens numéricos: ', token_ids)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Frase:  I really loved that movie!\n","Tokens:  ['I', 'really', 'loved', 'that', 'movie', '!']\n","Tokens numéricos:  [146, 1541, 3097, 1115, 2523, 106]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GU6ZO0xWEuL4"},"source":["BERT model needs one special format data to works fine. To optimize computation time, BERT works with data vector of same lenght it's important to take care about this, cause we will add some padding tokens to maintain the same lenght. To exclude this parts of analysis we will use attention_mask to tell BERT model will know if this part is relevant or not."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDqgDsUqW0fO","executionInfo":{"status":"ok","timestamp":1618737389830,"user_tz":-120,"elapsed":684,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}},"outputId":"20ca369a-98b6-4260-c32e-8279e19d850d"},"source":["# Codification to BERT example\n","encoding = tokenizer.encode_plus(\n","    sample_txt,\n","    max_length = 10,\n","    truncation = True,\n","    add_special_tokens = True,\n","    return_token_type_ids = False,\n","    padding = True,\n","    return_attention_mask = True,\n","    return_tensors = 'pt'\n",")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QfE50cS2X8uu","executionInfo":{"status":"ok","timestamp":1618737392765,"user_tz":-120,"elapsed":1429,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}},"outputId":"cabfdf3f-8249-48bd-9d45-51da32c3f563"},"source":["encoding.keys()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'attention_mask'])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"IJSA958ZEuL5"},"source":["Printing encoding example:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"er5DlrQ9X-mG","executionInfo":{"status":"ok","timestamp":1618737394922,"user_tz":-120,"elapsed":816,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}},"outputId":"45ee1ff1-9168-4553-81e1-9a505a8d54cb"},"source":["print(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]))\n","print(encoding['input_ids'][0])\n","print(encoding['attention_mask'][0])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["['[CLS]', 'I', 'really', 'loved', 'that', 'movie', '!', '[SEP]', '[PAD]', '[PAD]']\n","tensor([ 101,  146, 1541, 3097, 1115, 2523,  106,  102,    0,    0])\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l0q0qDJ3EuL5"},"source":["BERT model needs structured data that derivate from torch.Dataset where we only need to implement 3 functions:\n","- __init__: initialitzation dataset\n","- __len__: return len of dataset\n","- __getitem__: return data that will enter to BERT model."]},{"cell_type":"code","metadata":{"id":"pqv-DZtzYQhm","executionInfo":{"status":"ok","timestamp":1618737396977,"user_tz":-120,"elapsed":847,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}}},"source":["# DATASET creation\n","\n","class IMDBDataset(Dataset):\n","\n","    def __init__(self,reviews,labels,tokenizer,max_len):\n","        self.reviews = reviews\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.reviews)\n","\n","    def __getitem__(self, item):\n","        review = str(self.reviews[item])\n","        label = self.labels[item]\n","        encoding = tokenizer.encode_plus(\n","            review,\n","            max_length = self.max_len,\n","            truncation = True,\n","            add_special_tokens = True,\n","            return_token_type_ids = False,\n","            padding = True,\n","            return_attention_mask = True,\n","            return_tensors = 'pt'\n","            )\n","\n","\n","        return {\n","              'review': review,\n","              'input_ids': encoding['input_ids'].flatten(),\n","              'attention_mask': encoding['attention_mask'].flatten(),\n","              'label': torch.tensor(label, dtype=torch.long)\n","          } \n","\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xoJsu7G-EuL6"},"source":["To have clear code we define function that will create dataset from  DataLoader."]},{"cell_type":"code","metadata":{"id":"NaYe0FsoaNAv","executionInfo":{"status":"ok","timestamp":1618737400590,"user_tz":-120,"elapsed":1132,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}}},"source":["# Data loader:\n","\n","def data_loader(df, tokenizer, max_len, batch_size):\n","    dataset = IMDBDataset(  \n","      reviews = df.review.to_numpy(),\n","      labels = df.label.to_numpy(),\n","      tokenizer = tokenizer,\n","      max_len = MAX_LEN)  \n","\n","    return DataLoader(dataset, batch_size = BATCH_SIZE, num_workers = 4)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oTRr0F3LEuL7"},"source":["In all machine learning algorithms it's important to split dataset on train and test. Train dataset it's used for train model and test dataset will be the data that we will use for define how good it's our model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m9rNI4Tsa5MZ","executionInfo":{"status":"ok","timestamp":1618737403863,"user_tz":-120,"elapsed":1171,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}},"outputId":"872954e3-8f0e-4607-fbf4-bfebbe424dce"},"source":["df_train, df_test = train_test_split(df, test_size = 0.2, random_state=RANDOM_SEED)\n","\n","train_data_loader = data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n","test_data_loader = data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Ld4Q0auGEuL7"},"source":["## Modelling\n","It's easy to create one new model based on BERT, we conli should define our base model (BERT), and added layers. In this case we add dropout layer that will make that our model avoid overfits by hidding some neural perceptrons on each time that we load data.\n","Create neural network that will be the power of our model, that will recibe BERT output as input and will have 2 neurons as output, one per each class to predict."]},{"cell_type":"code","metadata":{"id":"VOnca-sVbhPX","executionInfo":{"status":"ok","timestamp":1618737406087,"user_tz":-120,"elapsed":995,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}}},"source":["class BERTSentimentClassifier(nn.Module):\n","\n","    def __init__(self, n_classes):\n","        super(BERTSentimentClassifier, self).__init__()\n","        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","        self.drop = nn.Dropout(p=0.3)\n","        self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(\n","            input_ids = input_ids,\n","            attention_mask = attention_mask\n","        )\n","        drop_output = self.drop(outputs[1])\n","        output = self.linear(drop_output)\n","        return output\n","\n","\n"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jQhZgwvTEuL8"},"source":["Create Model and send it to device.\n","\n","Printing model you can see that BERT model implements a lot of layers and at the end were our two added layers"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["93945a4151bd4f75970f68c677e47804","7311ea8dd5624cd595cec1d9092f1fd1","8b449fc63bec4e85ae9956d27f50b48f","58101c72078e4debbeec620b673a5c79","cf9cb125ebec4d84a9f1b92369e228ea","7047b54ed3eb44f9b254b711f0d6a9c7","b2674396e7924eb78cc76151f991926f","21c60397c2dd4f0f9238028210843bec","49a1682522b54d75b81439f909b61141","cee3a917e8054873b63db48de4ed1228","e0743d771ab146ae92b0b48702ced581","56ae9f475c634386804597808a8a3574","b27de67ee9d44fdd814b502fd65cab0b","07160fcf72574e358f724b70e207ea17","8231499765a84865b4203270ec42030f","b1501470638946098ead819f82f422e4"]},"id":"CanuNncKdXaF","executionInfo":{"status":"ok","timestamp":1618737434537,"user_tz":-120,"elapsed":26871,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}},"outputId":"2d3f2774-9e08-435c-ae86-963b1bd3a889"},"source":["model = BERTSentimentClassifier(NCLASSES)\n","model = model.to(device)\n","print(model)"],"execution_count":17,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93945a4151bd4f75970f68c677e47804","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49a1682522b54d75b81439f909b61141","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","BERTSentimentClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (drop): Dropout(p=0.3, inplace=False)\n","  (linear): Linear(in_features=768, out_features=2, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qh6rkR_-Ne-L"},"source":["Let's define some interesting parameters that we will use to train our model:\n","- EPOCHS: number of iterations that our model will train with whole training dataset.\n","- Optimizer: optimizer function.\n","- total_steps: total number of examples to process.\n","- scheduler: this will change our lerning rate of our optimizer to improve our error, and have more robust model.\n","- loss_fn: function that we want to optimize"]},{"cell_type":"code","metadata":{"id":"Mansb6pfdn_H","executionInfo":{"status":"ok","timestamp":1618737434538,"user_tz":-120,"elapsed":23721,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}}},"source":["# ENTRENAMIENTO\n","EPOCHS = 5\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","total_steps = len(train_data_loader) * EPOCHS\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps = 0,\n","    num_training_steps = total_steps\n",")\n","loss_fn = nn.CrossEntropyLoss().to(device)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tuapfw8wEuL9"},"source":["Train fucntion and validation function basically the same only with two diferences, when we train we should put our model to train mode, wich let us make some weight rectification, mean while when we evaluate our model run as eval mode, wich freezze our weights.\n","Last  5 th rows of trrain_epoch's function are dedicated to upgrade this weights."]},{"cell_type":"code","metadata":{"id":"ihoNOG0Zeuhz","executionInfo":{"status":"ok","timestamp":1618737435679,"user_tz":-120,"elapsed":1131,"user":{"displayName":"Pol Renau Larrodé","photoUrl":"","userId":"15215146478217147182"}}},"source":["# TRAINING EPOCH\n","def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n","    model = model.train()\n","    losses = []\n","    correct_predictions = 0\n","    for batch in data_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['label'].to(device)\n","        outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n","        _, preds = torch.max(outputs, dim=1)\n","        loss = loss_fn(outputs, labels)\n","        correct_predictions += torch.sum(preds == labels)\n","        losses.append(loss.item())\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","    return correct_predictions.double()/n_examples, np.mean(losses)\n","\n","# EVALUATING MODEL\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model = model.eval()\n","    losses = []\n","    correct_predictions = 0\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['label'].to(device)\n","            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n","            _, preds = torch.max(outputs, dim=1)\n","            loss = loss_fn(outputs, labels)\n","            correct_predictions += torch.sum(preds == labels)\n","            losses.append(loss.item())\n","    return correct_predictions.double()/n_examples, np.mean(losses)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lhn8UzTPEuL-"},"source":["For each epoch train model and validate it, printing results on screen. If you want to let our alogtrithm training a lot of time and only store best of them, you should store model at the end of each epoh where the results are better, in this case lower Loss and high accuracy."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0aJRM-ZiXtp","outputId":"e8a0805a-f7d8-4eed-98ec-6a7dc33bf045"},"source":["# TRAINING!!!\n","\n","for epoch in range(EPOCHS):\n","    print('Epoch {} de {}'.format(epoch+1, EPOCHS))\n","    print('------------------')\n","    train_acc, train_loss = train_epoch(\n","      model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train)\n","    )\n","    test_acc, test_loss = eval_model(\n","      model, test_data_loader, loss_fn, device, len(df_test)\n","    )\n","    print('Training: Loss: {}, accuracy: {}'.format(train_loss, train_acc))\n","    print('Validation: Loss: {}, accuracy: {}'.format(test_loss, test_acc))\n","    print('')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1 de 5\n","------------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Training: Loss: 0.40202889354526994, accuracy: 0.816375\n","Validation: Loss: 0.2888660610467195, accuracy: 0.8855000000000001\n","\n","Epoch 2 de 5\n","------------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Training: Loss: 0.18809325058874674, accuracy: 0.933125\n","Validation: Loss: 0.4618920519771054, accuracy: 0.883\n","\n","Epoch 3 de 5\n","------------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"bt0yQ3j_EuL_"},"source":["This funtion recibe as input one review and predict if it's possitive or negative sentiment as output."]},{"cell_type":"code","metadata":{"id":"QTG7YKnVjDA-"},"source":["def classifySentiment(review_text):\n","    encoding_review = tokenizer.encode_plus(\n","      review_text,\n","      max_length = MAX_LEN,\n","      truncation = True,\n","      add_special_tokens = True,\n","      return_token_type_ids = False,\n","      padding = True,\n","      return_attention_mask = True,\n","      return_tensors = 'pt'\n","      )\n","\n","    input_ids = encoding_review['input_ids'].to(device)\n","    attention_mask = encoding_review['attention_mask'].to(device)\n","    output = model(input_ids, attention_mask)\n","    _, prediction = torch.max(output, dim=1)\n","    print(\"\\n\".join(wrap(review_text)))\n","    if prediction:\n","    print('Predicted sentiment: 5')\n","    else:\n","    print('Predicted sentiment: 1')\n","\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oe0im1_alf4K"},"source":["review_text = \"Avengers: Infinity War at least had the good taste to abstain from Jeremy Renner. No such luck in Endgame.\"\n","\n","classifySentiment(review_text)"],"execution_count":null,"outputs":[]}]}
